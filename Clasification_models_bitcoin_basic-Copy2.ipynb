{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import numbers\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are loading the bitcoin data, but also gold and nasdaq, in case we want to use them\n",
    "data_for_use = pd.read_csv('./bitcoin_data_for_use_v3.csv', index_col=0)\n",
    "data_gold = pd.read_csv('./gold_data_for_use.csv', index_col=0)\n",
    "data_nasdaq = pd.read_csv('./nasdaq_data_for_use.csv', index_col=0)\n",
    "\n",
    "data_for_use_w_gold = data_for_use.merge(data_gold, left_index=True, right_index=True)\n",
    "data_for_use_w_nasdaq = data_for_use.merge(data_nasdaq, left_index=True, right_index=True)\n",
    "\n",
    "data_for_use_w_all = data_for_use_w_gold.merge(data_nasdaq, left_index=True, right_index=True)\n",
    "\n",
    "data_for_use_basic = data_for_use.drop(labels=['dia',\n",
    "                    'varV0','varV1','varV2','varV3','varV4','varV5','varV6','varV7','varV8','varV9',\n",
    "                    'varV10','varV11','varV12','varV13','varV14','varV15','varV16','varV17','varV18','varV19',\n",
    "                    'varV20','varV21','varV22','varV23','varV24','varV25','varV26','varV27','varV28','varV29'\n",
    "                    ], axis=1)\n",
    "\n",
    "\n",
    "data_for_use_bone_deep = data_for_use_basic.drop(labels=['varPm93',\n",
    "                    'varPm123','varPm153','varPm183','varPm213','varPm243','varPm273','varPm303','varPm333',\n",
    "                    'varPs30','varPs37','varPs44','varPs51','varPs58','varPs65','varPs72','varPs79','varPs86'\n",
    "                    ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varP0</th>\n",
       "      <th>varP1</th>\n",
       "      <th>varP2</th>\n",
       "      <th>varP3</th>\n",
       "      <th>varP4</th>\n",
       "      <th>varP5</th>\n",
       "      <th>varP6</th>\n",
       "      <th>varP7</th>\n",
       "      <th>varP8</th>\n",
       "      <th>varP9</th>\n",
       "      <th>...</th>\n",
       "      <th>varPm123</th>\n",
       "      <th>varPm153</th>\n",
       "      <th>varPm183</th>\n",
       "      <th>varPm213</th>\n",
       "      <th>varPm243</th>\n",
       "      <th>varPm273</th>\n",
       "      <th>varPm303</th>\n",
       "      <th>varPm333</th>\n",
       "      <th>outlier</th>\n",
       "      <th>subida</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.014696</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>-0.084229</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.028445</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183043</td>\n",
       "      <td>0.102536</td>\n",
       "      <td>0.131128</td>\n",
       "      <td>-0.060556</td>\n",
       "      <td>-0.054907</td>\n",
       "      <td>-0.068731</td>\n",
       "      <td>0.146342</td>\n",
       "      <td>-0.152457</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>-0.002063</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.014696</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>-0.084229</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.028445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192818</td>\n",
       "      <td>0.102514</td>\n",
       "      <td>0.134863</td>\n",
       "      <td>-0.054992</td>\n",
       "      <td>-0.057860</td>\n",
       "      <td>-0.081400</td>\n",
       "      <td>0.215208</td>\n",
       "      <td>-0.139772</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>-0.007907</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.014696</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>-0.084229</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184701</td>\n",
       "      <td>0.077969</td>\n",
       "      <td>0.162987</td>\n",
       "      <td>-0.049962</td>\n",
       "      <td>-0.093926</td>\n",
       "      <td>-0.044459</td>\n",
       "      <td>0.202201</td>\n",
       "      <td>-0.173499</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>0.007163</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.014696</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>-0.084229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203476</td>\n",
       "      <td>0.048931</td>\n",
       "      <td>0.208760</td>\n",
       "      <td>-0.021020</td>\n",
       "      <td>-0.100629</td>\n",
       "      <td>-0.075179</td>\n",
       "      <td>0.272441</td>\n",
       "      <td>-0.241372</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>-0.002611</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.014696</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182999</td>\n",
       "      <td>0.047772</td>\n",
       "      <td>0.192408</td>\n",
       "      <td>-0.049361</td>\n",
       "      <td>-0.062588</td>\n",
       "      <td>-0.077178</td>\n",
       "      <td>0.234350</td>\n",
       "      <td>-0.244859</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21</th>\n",
       "      <td>-0.050185</td>\n",
       "      <td>-0.093449</td>\n",
       "      <td>-0.021089</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.008157</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>0.047359</td>\n",
       "      <td>-0.023884</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307969</td>\n",
       "      <td>-0.011426</td>\n",
       "      <td>-0.027993</td>\n",
       "      <td>0.819894</td>\n",
       "      <td>0.296122</td>\n",
       "      <td>0.294667</td>\n",
       "      <td>0.420558</td>\n",
       "      <td>0.264591</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-22</th>\n",
       "      <td>0.070793</td>\n",
       "      <td>-0.050185</td>\n",
       "      <td>-0.093449</td>\n",
       "      <td>-0.021089</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.008157</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>0.047359</td>\n",
       "      <td>-0.023884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274826</td>\n",
       "      <td>-0.054379</td>\n",
       "      <td>-0.048679</td>\n",
       "      <td>0.743330</td>\n",
       "      <td>0.420131</td>\n",
       "      <td>0.265584</td>\n",
       "      <td>0.400979</td>\n",
       "      <td>0.219860</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-23</th>\n",
       "      <td>0.030306</td>\n",
       "      <td>0.070793</td>\n",
       "      <td>-0.050185</td>\n",
       "      <td>-0.093449</td>\n",
       "      <td>-0.021089</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.008157</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>0.047359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.319474</td>\n",
       "      <td>-0.031845</td>\n",
       "      <td>-0.026437</td>\n",
       "      <td>0.690406</td>\n",
       "      <td>0.351016</td>\n",
       "      <td>0.242234</td>\n",
       "      <td>0.466289</td>\n",
       "      <td>0.217606</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-24</th>\n",
       "      <td>-0.045781</td>\n",
       "      <td>0.030306</td>\n",
       "      <td>0.070793</td>\n",
       "      <td>-0.050185</td>\n",
       "      <td>-0.093449</td>\n",
       "      <td>-0.021089</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.008157</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226667</td>\n",
       "      <td>-0.031976</td>\n",
       "      <td>0.058981</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.309128</td>\n",
       "      <td>0.316711</td>\n",
       "      <td>0.432640</td>\n",
       "      <td>0.215800</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-25</th>\n",
       "      <td>-0.002875</td>\n",
       "      <td>-0.045781</td>\n",
       "      <td>0.030306</td>\n",
       "      <td>0.070793</td>\n",
       "      <td>-0.050185</td>\n",
       "      <td>-0.093449</td>\n",
       "      <td>-0.021089</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.008157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216349</td>\n",
       "      <td>-0.111232</td>\n",
       "      <td>0.109284</td>\n",
       "      <td>0.535708</td>\n",
       "      <td>0.224282</td>\n",
       "      <td>0.541462</td>\n",
       "      <td>0.256068</td>\n",
       "      <td>0.267280</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2095 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               varP0     varP1     varP2     varP3     varP4     varP5  \\\n",
       "2016-01-01  0.008749  0.009252 -0.014696  0.025348 -0.001287  0.013298   \n",
       "2016-01-02 -0.002063  0.008749  0.009252 -0.014696  0.025348 -0.001287   \n",
       "2016-01-03 -0.007907 -0.002063  0.008749  0.009252 -0.014696  0.025348   \n",
       "2016-01-04  0.007163 -0.007907 -0.002063  0.008749  0.009252 -0.014696   \n",
       "2016-01-05 -0.002611  0.007163 -0.007907 -0.002063  0.008749  0.009252   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2021-09-21 -0.050185 -0.093449 -0.021089  0.021386 -0.010795 -0.008157   \n",
       "2021-09-22  0.070793 -0.050185 -0.093449 -0.021089  0.021386 -0.010795   \n",
       "2021-09-23  0.030306  0.070793 -0.050185 -0.093449 -0.021089  0.021386   \n",
       "2021-09-24 -0.045781  0.030306  0.070793 -0.050185 -0.093449 -0.021089   \n",
       "2021-09-25 -0.002875 -0.045781  0.030306  0.070793 -0.050185 -0.093449   \n",
       "\n",
       "               varP6     varP7     varP8     varP9  ...  varPm123  varPm153  \\\n",
       "2016-01-01 -0.084229  0.001468  0.028445  0.013352  ... -0.183043  0.102536   \n",
       "2016-01-02  0.013298 -0.084229  0.001468  0.028445  ... -0.192818  0.102514   \n",
       "2016-01-03 -0.001287  0.013298 -0.084229  0.001468  ... -0.184701  0.077969   \n",
       "2016-01-04  0.025348 -0.001287  0.013298 -0.084229  ... -0.203476  0.048931   \n",
       "2016-01-05 -0.014696  0.025348 -0.001287  0.013298  ... -0.182999  0.047772   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2021-09-21  0.023015  0.047359 -0.023884  0.019066  ... -0.307969 -0.011426   \n",
       "2021-09-22 -0.008157  0.023015  0.047359 -0.023884  ... -0.274826 -0.054379   \n",
       "2021-09-23 -0.010795 -0.008157  0.023015  0.047359  ... -0.319474 -0.031845   \n",
       "2021-09-24  0.021386 -0.010795 -0.008157  0.023015  ... -0.226667 -0.031976   \n",
       "2021-09-25 -0.021089  0.021386 -0.010795 -0.008157  ... -0.216349 -0.111232   \n",
       "\n",
       "            varPm183  varPm213  varPm243  varPm273  varPm303  varPm333  \\\n",
       "2016-01-01  0.131128 -0.060556 -0.054907 -0.068731  0.146342 -0.152457   \n",
       "2016-01-02  0.134863 -0.054992 -0.057860 -0.081400  0.215208 -0.139772   \n",
       "2016-01-03  0.162987 -0.049962 -0.093926 -0.044459  0.202201 -0.173499   \n",
       "2016-01-04  0.208760 -0.021020 -0.100629 -0.075179  0.272441 -0.241372   \n",
       "2016-01-05  0.192408 -0.049361 -0.062588 -0.077178  0.234350 -0.244859   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2021-09-21 -0.027993  0.819894  0.296122  0.294667  0.420558  0.264591   \n",
       "2021-09-22 -0.048679  0.743330  0.420131  0.265584  0.400979  0.219860   \n",
       "2021-09-23 -0.026437  0.690406  0.351016  0.242234  0.466289  0.217606   \n",
       "2021-09-24  0.058981  0.512089  0.309128  0.316711  0.432640  0.215800   \n",
       "2021-09-25  0.109284  0.535708  0.224282  0.541462  0.256068  0.267280   \n",
       "\n",
       "            outlier  subida  \n",
       "2016-01-01    False   False  \n",
       "2016-01-02    False   False  \n",
       "2016-01-03    False    True  \n",
       "2016-01-04    False   False  \n",
       "2016-01-05    False   False  \n",
       "...             ...     ...  \n",
       "2021-09-21    False    True  \n",
       "2021-09-22    False    True  \n",
       "2021-09-23    False   False  \n",
       "2021-09-24    False   False  \n",
       "2021-09-25    False    True  \n",
       "\n",
       "[2095 rows x 50 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_use_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui se marca lo que se va a usar de verdad para el resto del programa\n",
    "data_for_use = data_for_use_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partimos los datos en train y test por fechas, sin que sea aleatorio, que es como funcionaría\n",
    "# en la realidad el modelo\n",
    "data_no_test, data_test = train_test_split(data_for_use, shuffle=False, test_size=0.2)\n",
    "\n",
    "# Partimos los datos de train nuevamente en validation y train\n",
    "data_train, data_valid = train_test_split(data_no_test, shuffle=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos la variable que queremos predecir\n",
    "data_train = data_train[data_train['outlier']==0]\n",
    "X_train=data_train.drop(labels=['subida','outlier'], axis=1)\n",
    "y_train=data_train['subida']\n",
    "\n",
    "X_valid=data_valid.drop(labels=['subida','outlier'], axis=1)\n",
    "y_valid=data_valid['subida']\n",
    "\n",
    "X_test=data_test.drop(labels=['subida','outlier'], axis=1)\n",
    "y_test=data_test['subida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5684210526315789"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Miramos cuántos días sube el bitcoin sobre el total en cada uno de los segmentos\n",
    "# esto nos servirá para hacernos una idea de cómo de buenos son los resultados de los modelos\n",
    "y_train.sum()/y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4851190476190476"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.sum()/y_valid.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5513126491646778"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()/y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenemos que hacer one-hot encoding sobre la columna \"dia\"\n",
    "#Sobre el resto aplicamos el standardscaler\n",
    "\n",
    "number_columns = data_for_use.select_dtypes('number').columns\n",
    "\n",
    "t=[\n",
    "    #('dia', \n",
    "    #OneHotEncoder(handle_unknown='ignore'),\n",
    "    #['dia']),\n",
    "    ('scaler', StandardScaler(),number_columns)\n",
    "    ]\n",
    "\n",
    "ct = ColumnTransformer(transformers=t, remainder='passthrough')\n",
    "\n",
    "#data_for_use_t=ct.fit_transform(data_for_use)\n",
    "\n",
    "#X_train=ct.fit_transform(X_train)\n",
    "#X_valid=ct.transform(X_valid)\n",
    "#X_test=ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation (model, X_train_wfv, y_train_wfv, X_test_wfv, y_test_wfv):\n",
    "    y_pred_wfv = list()\n",
    "    for i in range(len(y_test_wfv)):\n",
    "        X_train_wfv_ct=ct.fit_transform(X_train_wfv)\n",
    "        model.fit(X_train_wfv_ct, y_train_wfv)\n",
    "        X_test_wfv_ct=ct.transform(X_test_wfv)\n",
    "        y_pred_next = model.predict(X_test_wfv_ct[i:i+1])\n",
    "        y_pred_wfv.append(y_pred_next[0])\n",
    "        X_train_wfv=X_train_wfv.append(X_test_wfv[i:i+1])\n",
    "        y_train_wfv=y_train_wfv.append(pd.Series(y_test_wfv[i]))\n",
    "    \n",
    "    return metrics.confusion_matrix(y_test_wfv, y_pred_wfv), metrics.f1_score(y_test_wfv,y_pred_wfv), metrics.accuracy_score(y_test_wfv,y_pred_wfv)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "\n",
    "X_train_c=X_train.copy()\n",
    "y_train_c=y_train.copy()\n",
    "X_valid_c=X_valid.copy()\n",
    "y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "conf_mat, f1sc, accsc = walk_forward_validation(lr, X_train_c, y_train_c, X_valid_c, y_valid_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 32 141]\n",
      " [ 26 137]] 0.6213151927437642 0.5029761904761905\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de vecinos aplicado: 1\n",
      "[[ 68 105]\n",
      " [ 60 103]] 0.5552560646900269 0.5089285714285714\n",
      "Numero de vecinos aplicado: 2\n",
      "[[105  68]\n",
      " [ 99  64]] 0.4338983050847458 0.5029761904761905\n",
      "Numero de vecinos aplicado: 3\n",
      "[[ 55 118]\n",
      " [ 53 110]] 0.5626598465473145 0.49107142857142855\n",
      "Numero de vecinos aplicado: 4\n",
      "[[88 85]\n",
      " [77 86]] 0.5149700598802395 0.5178571428571429\n",
      "Numero de vecinos aplicado: 5\n",
      "[[ 44 129]\n",
      " [ 40 123]] 0.5927710843373494 0.49702380952380953\n",
      "Numero de vecinos aplicado: 6\n",
      "[[ 72 101]\n",
      " [ 65  98]] 0.5414364640883977 0.5059523809523809\n",
      "Numero de vecinos aplicado: 7\n",
      "[[ 34 139]\n",
      " [ 32 131]] 0.605080831408776 0.49107142857142855\n",
      "Numero de vecinos aplicado: 8\n",
      "[[ 56 117]\n",
      " [ 46 117]] 0.5894206549118388 0.5148809523809523\n",
      "Numero de vecinos aplicado: 9\n",
      "[[ 30 143]\n",
      " [ 31 132]] 0.6027397260273972 0.48214285714285715\n",
      "Numero de vecinos aplicado: 10\n",
      "[[ 45 128]\n",
      " [ 41 122]] 0.5907990314769975 0.49702380952380953\n",
      "Numero de vecinos aplicado: 11\n",
      "[[ 26 147]\n",
      " [ 26 137]] 0.6129753914988815 0.4851190476190476\n",
      "Numero de vecinos aplicado: 12\n",
      "[[ 38 135]\n",
      " [ 33 130]] 0.6074766355140186 0.5\n",
      "Numero de vecinos aplicado: 13\n",
      "[[ 18 155]\n",
      " [ 18 145]] 0.6263498920086392 0.4851190476190476\n",
      "Numero de vecinos aplicado: 14\n",
      "[[ 32 141]\n",
      " [ 24 139]] 0.6275395033860045 0.5089285714285714\n",
      "Numero de vecinos aplicado: 15\n",
      "[[ 16 157]\n",
      " [ 16 147]] 0.6295503211991433 0.4851190476190476\n",
      "Numero de vecinos aplicado: 16\n",
      "[[ 23 150]\n",
      " [ 22 141]] 0.6211453744493393 0.4880952380952381\n",
      "Numero de vecinos aplicado: 17\n",
      "[[ 13 160]\n",
      " [ 13 150]] 0.6342494714587738 0.4851190476190476\n",
      "Numero de vecinos aplicado: 18\n",
      "[[ 19 154]\n",
      " [ 20 143]] 0.6217391304347826 0.48214285714285715\n",
      "Numero de vecinos aplicado: 19\n",
      "[[ 13 160]\n",
      " [ 12 151]] 0.6371308016877637 0.4880952380952381\n",
      "Numero de vecinos aplicado: 20\n",
      "[[ 19 154]\n",
      " [ 19 144]] 0.6247288503253796 0.4851190476190476\n",
      "Numero de vecinos aplicado: 21\n",
      "[[ 13 160]\n",
      " [ 11 152]] 0.6399999999999999 0.49107142857142855\n",
      "Numero de vecinos aplicado: 22\n",
      "[[ 17 156]\n",
      " [ 17 146]] 0.6279569892473118 0.4851190476190476\n",
      "Numero de vecinos aplicado: 23\n",
      "[[ 11 162]\n",
      " [ 10 153]] 0.6401673640167365 0.4880952380952381\n",
      "Numero de vecinos aplicado: 24\n",
      "[[ 16 157]\n",
      " [ 17 146]] 0.626609442060086 0.48214285714285715\n",
      "Numero de vecinos aplicado: 25\n",
      "[[  9 164]\n",
      " [ 13 150]] 0.6289308176100629 0.4732142857142857\n",
      "Numero de vecinos aplicado: 26\n",
      "[[ 13 160]\n",
      " [ 14 149]] 0.6313559322033899 0.48214285714285715\n",
      "Numero de vecinos aplicado: 27\n",
      "[[  8 165]\n",
      " [ 10 153]] 0.6361746361746361 0.4791666666666667\n",
      "Numero de vecinos aplicado: 28\n",
      "[[ 10 163]\n",
      " [ 12 151]] 0.6331236897274632 0.4791666666666667\n",
      "Numero de vecinos aplicado: 29\n",
      "[[ 10 163]\n",
      " [  7 156]] 0.6473029045643154 0.49404761904761907\n",
      "Numero de vecinos aplicado: 30\n",
      "[[ 13 160]\n",
      " [  7 156]] 0.651356993736952 0.5029761904761905\n"
     ]
    }
   ],
   "source": [
    "for vecinos in range(1,31):\n",
    "    kn = KNeighborsClassifier(n_neighbors=vecinos)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(kn, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Numero de vecinos aplicado: \" + str(vecinos))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxima profundidad aplicada: 1\n",
      "[[ 13 160]\n",
      " [  9 154]] 0.6457023060796646 0.49702380952380953\n",
      "Maxima profundidad aplicada: 2\n",
      "[[ 29 144]\n",
      " [ 27 136]] 0.6139954853273139 0.49107142857142855\n",
      "Maxima profundidad aplicada: 3\n",
      "[[ 26 147]\n",
      " [ 27 136]] 0.6098654708520179 0.48214285714285715\n",
      "Maxima profundidad aplicada: 4\n",
      "[[ 35 138]\n",
      " [ 37 126]] 0.5901639344262295 0.4791666666666667\n",
      "Maxima profundidad aplicada: 5\n",
      "[[ 36 137]\n",
      " [ 33 130]] 0.6046511627906976 0.49404761904761907\n",
      "Maxima profundidad aplicada: 6\n",
      "[[ 39 134]\n",
      " [ 40 123]] 0.5857142857142857 0.48214285714285715\n",
      "Maxima profundidad aplicada: 7\n",
      "[[ 44 129]\n",
      " [ 42 121]] 0.5859564164648909 0.49107142857142855\n",
      "Maxima profundidad aplicada: 8\n",
      "[[ 45 128]\n",
      " [ 43 120]] 0.583941605839416 0.49107142857142855\n",
      "Maxima profundidad aplicada: 9\n",
      "[[ 50 123]\n",
      " [ 50 113]] 0.5664160401002506 0.4851190476190476\n",
      "Maxima profundidad aplicada: 10\n",
      "[[ 55 118]\n",
      " [ 47 116]] 0.584382871536524 0.5089285714285714\n",
      "Maxima profundidad aplicada: 11\n",
      "[[ 62 111]\n",
      " [ 58 105]] 0.5540897097625329 0.49702380952380953\n",
      "Maxima profundidad aplicada: 12\n",
      "[[ 68 105]\n",
      " [ 57 106]] 0.5668449197860963 0.5178571428571429\n",
      "Maxima profundidad aplicada: 13\n",
      "[[74 99]\n",
      " [69 94]] 0.5280898876404495 0.5\n",
      "Maxima profundidad aplicada: 14\n",
      "[[74 99]\n",
      " [72 91]] 0.5155807365439092 0.49107142857142855\n",
      "Maxima profundidad aplicada: 15\n",
      "[[82 91]\n",
      " [71 92]] 0.5317919075144508 0.5178571428571429\n",
      "Maxima profundidad aplicada: 16\n",
      "[[78 95]\n",
      " [74 89]] 0.5129682997118156 0.49702380952380953\n",
      "Maxima profundidad aplicada: 17\n",
      "[[76 97]\n",
      " [79 84]] 0.4883720930232558 0.47619047619047616\n",
      "Maxima profundidad aplicada: 18\n",
      "[[85 88]\n",
      " [77 86]] 0.5103857566765578 0.5089285714285714\n",
      "Maxima profundidad aplicada: 19\n",
      "[[77 96]\n",
      " [75 88]] 0.5072046109510087 0.49107142857142855\n",
      "Maxima profundidad aplicada: 20\n",
      "[[76 97]\n",
      " [75 88]] 0.5057471264367817 0.4880952380952381\n",
      "Maxima profundidad aplicada: 21\n",
      "[[78 95]\n",
      " [79 84]] 0.4912280701754386 0.48214285714285715\n",
      "Maxima profundidad aplicada: 22\n",
      "[[78 95]\n",
      " [85 78]] 0.4642857142857143 0.4642857142857143\n",
      "Maxima profundidad aplicada: 23\n",
      "[[75 98]\n",
      " [84 79]] 0.4647058823529412 0.4583333333333333\n",
      "Maxima profundidad aplicada: 24\n",
      "[[82 91]\n",
      " [77 86]] 0.5058823529411764 0.5\n",
      "Maxima profundidad aplicada: 25\n",
      "[[79 94]\n",
      " [80 83]] 0.48823529411764705 0.48214285714285715\n",
      "Maxima profundidad aplicada: 26\n",
      "[[84 89]\n",
      " [76 87]] 0.5132743362831858 0.5089285714285714\n",
      "Maxima profundidad aplicada: 27\n",
      "[[75 98]\n",
      " [81 82]] 0.47813411078717205 0.46726190476190477\n",
      "Maxima profundidad aplicada: 28\n",
      "[[74 99]\n",
      " [80 83]] 0.48115942028985503 0.46726190476190477\n",
      "Maxima profundidad aplicada: 29\n",
      "[[85 88]\n",
      " [82 81]] 0.4879518072289157 0.49404761904761907\n",
      "Maxima profundidad aplicada: 30\n",
      "[[80 93]\n",
      " [77 86]] 0.5029239766081872 0.49404761904761907\n"
     ]
    }
   ],
   "source": [
    "for profund in range(1,31):\n",
    "    dt = DecisionTreeClassifier(max_depth=profund)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(dt, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Maxima profundidad aplicada: \" + str(profund))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimadores aplicados: 1\n",
      "[[ 13 160]\n",
      " [  9 154]] 0.6457023060796646 0.49702380952380953\n",
      "Estimadores aplicados: 2\n",
      "[[ 29 144]\n",
      " [ 25 138]] 0.6202247191011236 0.49702380952380953\n",
      "Estimadores aplicados: 3\n",
      "[[ 45 128]\n",
      " [ 42 121]] 0.587378640776699 0.49404761904761907\n",
      "Estimadores aplicados: 4\n",
      "[[ 29 144]\n",
      " [ 21 142]] 0.6325167037861915 0.5089285714285714\n",
      "Estimadores aplicados: 5\n",
      "[[ 34 139]\n",
      " [ 25 138]] 0.6272727272727272 0.5119047619047619\n",
      "Estimadores aplicados: 6\n",
      "[[ 45 128]\n",
      " [ 38 125]] 0.6009615384615384 0.5059523809523809\n",
      "Estimadores aplicados: 7\n",
      "[[ 42 131]\n",
      " [ 39 124]] 0.5933014354066986 0.49404761904761907\n",
      "Estimadores aplicados: 8\n",
      "[[ 52 121]\n",
      " [ 45 118]] 0.5870646766169154 0.5059523809523809\n",
      "Estimadores aplicados: 9\n",
      "[[ 53 120]\n",
      " [ 45 118]] 0.5885286783042394 0.5089285714285714\n",
      "Estimadores aplicados: 10\n",
      "[[ 53 120]\n",
      " [ 44 119]] 0.5920398009950248 0.5119047619047619\n",
      "Estimadores aplicados: 11\n",
      "[[ 51 122]\n",
      " [ 42 121]] 0.5960591133004927 0.5119047619047619\n",
      "Estimadores aplicados: 12\n",
      "[[ 50 123]\n",
      " [ 47 116]] 0.5771144278606964 0.49404761904761907\n",
      "Estimadores aplicados: 13\n",
      "[[ 49 124]\n",
      " [ 46 117]] 0.5792079207920792 0.49404761904761907\n",
      "Estimadores aplicados: 14\n",
      "[[ 50 123]\n",
      " [ 42 121]] 0.5945945945945946 0.5089285714285714\n",
      "Estimadores aplicados: 15\n",
      "[[ 49 124]\n",
      " [ 45 118]] 0.5827160493827162 0.49702380952380953\n",
      "Estimadores aplicados: 16\n",
      "[[ 47 126]\n",
      " [ 47 116]] 0.5728395061728395 0.4851190476190476\n",
      "Estimadores aplicados: 17\n",
      "[[ 51 122]\n",
      " [ 47 116]] 0.5785536159600997 0.49702380952380953\n",
      "Estimadores aplicados: 18\n",
      "[[ 53 120]\n",
      " [ 48 115]] 0.577889447236181 0.5\n",
      "Estimadores aplicados: 19\n",
      "[[ 56 117]\n",
      " [ 44 119]] 0.5964912280701754 0.5208333333333334\n",
      "Estimadores aplicados: 20\n",
      "[[ 58 115]\n",
      " [ 48 115]] 0.5852417302798982 0.5148809523809523\n"
     ]
    }
   ],
   "source": [
    "for estimadores in range(1,21):\n",
    "    adab = AdaBoostClassifier(n_estimators=estimadores)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(adab, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Estimadores aplicados: \" + str(estimadores))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[493, 254],\n",
       "        [ 72, 847]]),\n",
       " 0.8386138613861388,\n",
       " 0.8043217286914766)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=12)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "dt.fit(X_train_c,y_train_c)\n",
    "y_predict=dt.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[660,  87],\n",
       "        [ 70, 849]]),\n",
       " 0.9153638814016173,\n",
       " 0.9057623049219687)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=15)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "dt.fit(X_train_c,y_train_c)\n",
    "y_predict=dt.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[312, 435],\n",
       "        [171, 748]]),\n",
       " 0.7117031398667935,\n",
       " 0.6362545018007203)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adab = AdaBoostClassifier(n_estimators=20)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "adab.fit(X_train_c,y_train_c)\n",
    "y_predict=adab.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[583, 164],\n",
       "        [306, 613]]),\n",
       " 0.722877358490566,\n",
       " 0.7178871548619448)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "kn.fit(X_train_c,y_train_c)\n",
    "y_predict=kn.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[401, 346],\n",
       "        [233, 686]]),\n",
       " 0.7032291132752434,\n",
       " 0.6524609843937575)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "kn.fit(X_train_c,y_train_c)\n",
    "y_predict=kn.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 72 116]\n",
      " [ 75 156]] 0.6202783300198808 0.5441527446300716\n"
     ]
    }
   ],
   "source": [
    "#Este es el código para ver cómo funciona el modelo de verdad (este es el resultado del basic)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=12)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "X_test_c=X_test.copy()\n",
    "y_test_c=y_test.copy()\n",
    "\n",
    "conf_mat, f1sc, accsc = walk_forward_validation(dt, X_train_c, y_train_c, X_test_c, y_test_c)\n",
    "\n",
    "print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
