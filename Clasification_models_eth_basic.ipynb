{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import numbers\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_for_use = pd.read_csv('./ethereum_data_for_use.csv', index_col=0)\n",
    "data_btc = pd.read_csv('./bitcoin_data_for_use_v2.csv', index_col=0)\n",
    "data_gold = pd.read_csv('./gold_data_for_use.csv', index_col=0)\n",
    "data_nasdaq = pd.read_csv('./nasdaq_data_for_use.csv', index_col=0)\n",
    "\n",
    "data_for_use_w_gold = data_for_use.merge(data_gold, left_index=True, right_index=True)\n",
    "data_for_use_w_nasdaq = data_for_use.merge(data_nasdaq, left_index=True, right_index=True)\n",
    "\n",
    "data_for_use_w_all = data_for_use_w_gold.merge(data_nasdaq, left_index=True, right_index=True)\n",
    "\n",
    "data_for_use_basic = data_for_use.drop(labels=['dia',\n",
    "                    'varV0','varV1','varV2','varV3','varV4','varV5','varV6','varV7','varV8','varV9',\n",
    "                    'varV10','varV11','varV12','varV13','varV14','varV15','varV16','varV17','varV18','varV19',\n",
    "                    'varV20','varV21','varV22','varV23','varV24','varV25','varV26','varV27','varV28','varV29'\n",
    "                    ], axis=1)\n",
    "\n",
    "data_for_use_bone_deep = data_for_use_basic.drop(labels=['varPm93',\n",
    "                    'varPm123','varPm153','varPm183','varPm213','varPm243','varPm273','varPm303','varPm333',\n",
    "                    'varPs30','varPs37','varPs44','varPs51','varPs58','varPs65','varPs72','varPs79','varPs86'\n",
    "                    ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varP0</th>\n",
       "      <th>varP1</th>\n",
       "      <th>varP2</th>\n",
       "      <th>varP3</th>\n",
       "      <th>varP4</th>\n",
       "      <th>varP5</th>\n",
       "      <th>varP6</th>\n",
       "      <th>varP7</th>\n",
       "      <th>varP8</th>\n",
       "      <th>varP9</th>\n",
       "      <th>...</th>\n",
       "      <th>varPm93</th>\n",
       "      <th>varPm123</th>\n",
       "      <th>varPm153</th>\n",
       "      <th>varPm183</th>\n",
       "      <th>varPm213</th>\n",
       "      <th>varPm243</th>\n",
       "      <th>varPm273</th>\n",
       "      <th>varPm303</th>\n",
       "      <th>varPm333</th>\n",
       "      <th>subida</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>-0.012482</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>-0.055221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132997</td>\n",
       "      <td>0.066791</td>\n",
       "      <td>-0.097853</td>\n",
       "      <td>-0.117164</td>\n",
       "      <td>0.475416</td>\n",
       "      <td>-0.198569</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>3.115174</td>\n",
       "      <td>1.649285</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>-0.012482</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098192</td>\n",
       "      <td>0.365030</td>\n",
       "      <td>-0.250245</td>\n",
       "      <td>-0.153574</td>\n",
       "      <td>0.470898</td>\n",
       "      <td>-0.156268</td>\n",
       "      <td>0.013959</td>\n",
       "      <td>3.268243</td>\n",
       "      <td>1.713224</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>0.160737</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>-0.012482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089714</td>\n",
       "      <td>0.176716</td>\n",
       "      <td>-0.102615</td>\n",
       "      <td>-0.165163</td>\n",
       "      <td>0.398357</td>\n",
       "      <td>-0.058631</td>\n",
       "      <td>-0.083002</td>\n",
       "      <td>3.483239</td>\n",
       "      <td>1.670204</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>0.156947</td>\n",
       "      <td>0.160737</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144038</td>\n",
       "      <td>0.064830</td>\n",
       "      <td>0.040654</td>\n",
       "      <td>-0.240687</td>\n",
       "      <td>0.495190</td>\n",
       "      <td>-0.125289</td>\n",
       "      <td>0.118582</td>\n",
       "      <td>2.769099</td>\n",
       "      <td>1.690495</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>-0.088627</td>\n",
       "      <td>0.156947</td>\n",
       "      <td>0.160737</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136739</td>\n",
       "      <td>0.069059</td>\n",
       "      <td>0.037658</td>\n",
       "      <td>-0.244379</td>\n",
       "      <td>0.487016</td>\n",
       "      <td>-0.069435</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>2.321890</td>\n",
       "      <td>2.004026</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21</th>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>-0.036545</td>\n",
       "      <td>0.042766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075806</td>\n",
       "      <td>0.027855</td>\n",
       "      <td>0.398158</td>\n",
       "      <td>-0.118883</td>\n",
       "      <td>0.711469</td>\n",
       "      <td>0.766659</td>\n",
       "      <td>0.137593</td>\n",
       "      <td>0.361917</td>\n",
       "      <td>0.278182</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-22</th>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>-0.036545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177400</td>\n",
       "      <td>-0.044863</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>-0.132750</td>\n",
       "      <td>0.565372</td>\n",
       "      <td>1.118351</td>\n",
       "      <td>-0.040660</td>\n",
       "      <td>0.475192</td>\n",
       "      <td>0.180620</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-23</th>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111221</td>\n",
       "      <td>-0.107467</td>\n",
       "      <td>0.483348</td>\n",
       "      <td>-0.105825</td>\n",
       "      <td>0.447609</td>\n",
       "      <td>1.012714</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.486636</td>\n",
       "      <td>0.153497</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-24</th>\n",
       "      <td>-0.070941</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247336</td>\n",
       "      <td>0.195316</td>\n",
       "      <td>0.386287</td>\n",
       "      <td>0.016020</td>\n",
       "      <td>0.128337</td>\n",
       "      <td>1.221561</td>\n",
       "      <td>0.097644</td>\n",
       "      <td>0.448854</td>\n",
       "      <td>0.109653</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-25</th>\n",
       "      <td>-0.002082</td>\n",
       "      <td>-0.070941</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265338</td>\n",
       "      <td>0.168635</td>\n",
       "      <td>0.360114</td>\n",
       "      <td>0.046888</td>\n",
       "      <td>0.228147</td>\n",
       "      <td>1.082951</td>\n",
       "      <td>0.225587</td>\n",
       "      <td>0.284171</td>\n",
       "      <td>0.130626</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1729 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               varP0     varP1     varP2     varP3     varP4     varP5  \\\n",
       "2017-01-01  0.025532 -0.023392 -0.014990  0.101601  0.048710 -0.014197   \n",
       "2017-01-02  0.025199  0.025532 -0.023392 -0.014990  0.101601  0.048710   \n",
       "2017-01-03  0.160737  0.025199  0.025532 -0.023392 -0.014990  0.101601   \n",
       "2017-01-04  0.156947  0.160737  0.025199  0.025532 -0.023392 -0.014990   \n",
       "2017-01-05 -0.088627  0.156947  0.160737  0.025199  0.025532 -0.023392   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2021-09-21 -0.065753 -0.111266 -0.029886  0.009851 -0.048374 -0.012167   \n",
       "2021-09-22  0.113382 -0.065753 -0.111266 -0.029886  0.009851 -0.048374   \n",
       "2021-09-23  0.025230  0.113382 -0.065753 -0.111266 -0.029886  0.009851   \n",
       "2021-09-24 -0.070941  0.025230  0.113382 -0.065753 -0.111266 -0.029886   \n",
       "2021-09-25 -0.002082 -0.070941  0.025230  0.113382 -0.065753 -0.111266   \n",
       "\n",
       "               varP6     varP7     varP8     varP9  ...   varPm93  varPm123  \\\n",
       "2017-01-01  0.013799 -0.012482  0.013901 -0.055221  ...  0.132997  0.066791   \n",
       "2017-01-02 -0.014197  0.013799 -0.012482  0.013901  ...  0.098192  0.365030   \n",
       "2017-01-03  0.048710 -0.014197  0.013799 -0.012482  ...  0.089714  0.176716   \n",
       "2017-01-04  0.101601  0.048710 -0.014197  0.013799  ...  0.144038  0.064830   \n",
       "2017-01-05 -0.014990  0.101601  0.048710 -0.014197  ...  0.136739  0.069059   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2021-09-21  0.054273  0.043725 -0.036545  0.042766  ... -0.075806  0.027855   \n",
       "2021-09-22 -0.012167  0.054273  0.043725 -0.036545  ... -0.177400 -0.044863   \n",
       "2021-09-23 -0.048374 -0.012167  0.054273  0.043725  ... -0.111221 -0.107467   \n",
       "2021-09-24  0.009851 -0.048374 -0.012167  0.054273  ... -0.247336  0.195316   \n",
       "2021-09-25 -0.029886  0.009851 -0.048374 -0.012167  ... -0.265338  0.168635   \n",
       "\n",
       "            varPm153  varPm183  varPm213  varPm243  varPm273  varPm303  \\\n",
       "2017-01-01 -0.097853 -0.117164  0.475416 -0.198569  0.116500  3.115174   \n",
       "2017-01-02 -0.250245 -0.153574  0.470898 -0.156268  0.013959  3.268243   \n",
       "2017-01-03 -0.102615 -0.165163  0.398357 -0.058631 -0.083002  3.483239   \n",
       "2017-01-04  0.040654 -0.240687  0.495190 -0.125289  0.118582  2.769099   \n",
       "2017-01-05  0.037658 -0.244379  0.487016 -0.069435  0.022594  2.321890   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2021-09-21  0.398158 -0.118883  0.711469  0.766659  0.137593  0.361917   \n",
       "2021-09-22  0.431826 -0.132750  0.565372  1.118351 -0.040660  0.475192   \n",
       "2021-09-23  0.483348 -0.105825  0.447609  1.012714  0.012766  0.486636   \n",
       "2021-09-24  0.386287  0.016020  0.128337  1.221561  0.097644  0.448854   \n",
       "2021-09-25  0.360114  0.046888  0.228147  1.082951  0.225587  0.284171   \n",
       "\n",
       "            varPm333  subida  \n",
       "2017-01-01  1.649285    True  \n",
       "2017-01-02  1.713224    True  \n",
       "2017-01-03  1.670204    True  \n",
       "2017-01-04  1.690495   False  \n",
       "2017-01-05  2.004026    True  \n",
       "...              ...     ...  \n",
       "2021-09-21  0.278182    True  \n",
       "2021-09-22  0.180620    True  \n",
       "2021-09-23  0.153497   False  \n",
       "2021-09-24  0.109653   False  \n",
       "2021-09-25  0.130626    True  \n",
       "\n",
       "[1729 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_use_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui se marca lo que se va a usar de verdad para el resto del programa\n",
    "data_for_use = data_for_use_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partimos los datos en train y test por fechas, sin que sea aleatorio, que es como funcionaría\n",
    "# en la realidad el modelo\n",
    "data_no_test, data_test = train_test_split(data_for_use, shuffle=False, test_size=0.2)\n",
    "\n",
    "# Partimos los datos de train nuevamente en validation y train\n",
    "data_train, data_valid = train_test_split(data_no_test, shuffle=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos la variable que queremos predecir\n",
    "X_train=data_train.drop(labels=['subida'], axis=1)\n",
    "y_train=data_train['subida']\n",
    "\n",
    "X_valid=data_valid.drop(labels=['subida'], axis=1)\n",
    "y_valid=data_valid['subida']\n",
    "\n",
    "X_test=data_test.drop(labels=['subida'], axis=1)\n",
    "y_test=data_test['subida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49909584086799275"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Miramos cuántos días sube el eth sobre el total en cada uno de los segmentos\n",
    "# esto nos servirá para hacernos una idea de cómo de buenos son los resultados de los modelos\n",
    "y_train.sum()/y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5703971119133574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.sum()/y_valid.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5664739884393064"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()/y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenemos que hacer one-hot encoding sobre la columna \"dia\"\n",
    "#Sobre el resto aplicamos el standardscaler\n",
    "\n",
    "number_columns = data_for_use.select_dtypes('number').columns\n",
    "\n",
    "t=[\n",
    "    #('dia', \n",
    "    #OneHotEncoder(handle_unknown='ignore'),\n",
    "    #['dia']),\n",
    "    ('scaler', StandardScaler(),number_columns)\n",
    "    ]\n",
    "\n",
    "ct = ColumnTransformer(transformers=t, remainder='passthrough')\n",
    "\n",
    "#data_for_use_t=ct.fit_transform(data_for_use)\n",
    "\n",
    "#X_train=ct.fit_transform(X_train)\n",
    "#X_valid=ct.transform(X_valid)\n",
    "#X_test=ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation (model, X_train_wfv, y_train_wfv, X_test_wfv, y_test_wfv):\n",
    "    y_pred_wfv = list()\n",
    "    for i in range(len(y_test_wfv)):\n",
    "        X_train_wfv_ct=ct.fit_transform(X_train_wfv)\n",
    "        model.fit(X_train_wfv_ct, y_train_wfv)\n",
    "        X_test_wfv_ct=ct.transform(X_test_wfv)\n",
    "        y_pred_next = model.predict(X_test_wfv_ct[i:i+1])\n",
    "        y_pred_wfv.append(y_pred_next[0])\n",
    "        X_train_wfv=X_train_wfv.append(X_test_wfv[i:i+1])\n",
    "        y_train_wfv=y_train_wfv.append(pd.Series(y_test_wfv[i]))\n",
    "    \n",
    "    return metrics.confusion_matrix(y_test_wfv, y_pred_wfv), metrics.f1_score(y_test_wfv,y_pred_wfv), metrics.accuracy_score(y_test_wfv,y_pred_wfv)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "\n",
    "X_train_c=X_train.copy()\n",
    "y_train_c=y_train.copy()\n",
    "X_valid_c=X_valid.copy()\n",
    "y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "conf_mat, f1sc, accsc = walk_forward_validation(lr, X_train_c, y_train_c, X_valid_c, y_valid_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57 62]\n",
      " [76 82]] 0.5430463576158941 0.5018050541516246\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de vecinos aplicado: 1\n",
      "[[60 59]\n",
      " [81 77]] 0.5238095238095238 0.49458483754512633\n",
      "Numero de vecinos aplicado: 2\n",
      "[[ 85  34]\n",
      " [124  34]] 0.3008849557522124 0.4296028880866426\n",
      "Numero de vecinos aplicado: 3\n",
      "[[58 61]\n",
      " [76 82]] 0.5448504983388704 0.5054151624548736\n",
      "Numero de vecinos aplicado: 4\n",
      "[[ 84  35]\n",
      " [106  52]] 0.42448979591836733 0.49097472924187724\n",
      "Numero de vecinos aplicado: 5\n",
      "[[61 58]\n",
      " [73 85]] 0.5647840531561461 0.5270758122743683\n",
      "Numero de vecinos aplicado: 6\n",
      "[[82 37]\n",
      " [93 65]] 0.5 0.5306859205776173\n",
      "Numero de vecinos aplicado: 7\n",
      "[[65 54]\n",
      " [67 91]] 0.6006600660066006 0.5631768953068592\n",
      "Numero de vecinos aplicado: 8\n",
      "[[82 37]\n",
      " [94 64]] 0.49420849420849416 0.5270758122743683\n",
      "Numero de vecinos aplicado: 9\n",
      "[[65 54]\n",
      " [68 90]] 0.5960264900662252 0.5595667870036101\n",
      "Numero de vecinos aplicado: 10\n",
      "[[77 42]\n",
      " [85 73]] 0.5347985347985347 0.5415162454873647\n",
      "Numero de vecinos aplicado: 11\n",
      "[[60 59]\n",
      " [74 84]] 0.5581395348837209 0.51985559566787\n",
      "Numero de vecinos aplicado: 12\n",
      "[[77 42]\n",
      " [89 69]] 0.5130111524163569 0.5270758122743683\n",
      "Numero de vecinos aplicado: 13\n",
      "[[62 57]\n",
      " [76 82]] 0.5521885521885521 0.51985559566787\n",
      "Numero de vecinos aplicado: 14\n",
      "[[74 45]\n",
      " [92 66]] 0.4907063197026022 0.5054151624548736\n",
      "Numero de vecinos aplicado: 15\n",
      "[[59 60]\n",
      " [75 83]] 0.5514950166112956 0.5126353790613718\n",
      "Numero de vecinos aplicado: 16\n",
      "[[71 48]\n",
      " [91 67]] 0.49084249084249076 0.4981949458483754\n",
      "Numero de vecinos aplicado: 17\n",
      "[[61 58]\n",
      " [71 87]] 0.5742574257425743 0.5342960288808665\n",
      "Numero de vecinos aplicado: 18\n",
      "[[75 44]\n",
      " [86 72]] 0.5255474452554744 0.5306859205776173\n",
      "Numero de vecinos aplicado: 19\n",
      "[[62 57]\n",
      " [74 84]] 0.5618729096989966 0.5270758122743683\n",
      "Numero de vecinos aplicado: 20\n",
      "[[72 47]\n",
      " [86 72]] 0.5198555956678701 0.51985559566787\n",
      "Numero de vecinos aplicado: 21\n",
      "[[61 58]\n",
      " [72 86]] 0.5695364238410596 0.5306859205776173\n",
      "Numero de vecinos aplicado: 22\n",
      "[[73 46]\n",
      " [89 69]] 0.5054945054945055 0.5126353790613718\n",
      "Numero de vecinos aplicado: 23\n",
      "[[63 56]\n",
      " [75 83]] 0.5589225589225589 0.5270758122743683\n",
      "Numero de vecinos aplicado: 24\n",
      "[[68 51]\n",
      " [85 73]] 0.5177304964539008 0.5090252707581228\n",
      "Numero de vecinos aplicado: 25\n",
      "[[51 68]\n",
      " [74 84]] 0.5419354838709678 0.48736462093862815\n",
      "Numero de vecinos aplicado: 26\n",
      "[[65 54]\n",
      " [86 72]] 0.5070422535211268 0.49458483754512633\n",
      "Numero de vecinos aplicado: 27\n",
      "[[59 60]\n",
      " [75 83]] 0.5514950166112956 0.5126353790613718\n",
      "Numero de vecinos aplicado: 28\n",
      "[[66 53]\n",
      " [88 70]] 0.498220640569395 0.49097472924187724\n",
      "Numero de vecinos aplicado: 29\n",
      "[[59 60]\n",
      " [78 80]] 0.5369127516778524 0.5018050541516246\n",
      "Numero de vecinos aplicado: 30\n",
      "[[68 51]\n",
      " [87 71]] 0.5071428571428571 0.5018050541516246\n"
     ]
    }
   ],
   "source": [
    "for vecinos in range(1,31):\n",
    "    kn = KNeighborsClassifier(n_neighbors=vecinos)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(kn, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Numero de vecinos aplicado: \" + str(vecinos))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grado aplicado: 1\n",
      "[[63 56]\n",
      " [77 81]] 0.5491525423728814 0.51985559566787\n",
      "Grado aplicado: 2\n",
      "[[44 75]\n",
      " [79 79]] 0.5064102564102564 0.44404332129963897\n",
      "Grado aplicado: 3\n",
      "[[ 41  78]\n",
      " [ 48 110]] 0.6358381502890174 0.5451263537906137\n"
     ]
    }
   ],
   "source": [
    "for grado in range(1,4):\n",
    "    sv = SVC(probability=True, kernel='poly', degree=grado)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(sv, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Grado aplicado: \" + str(grado))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxima profundidad aplicada: 1\n",
      "[[ 98  21]\n",
      " [112  46]] 0.4088888888888889 0.51985559566787\n",
      "Maxima profundidad aplicada: 2\n",
      "[[ 97  22]\n",
      " [110  48]] 0.42105263157894735 0.5234657039711191\n",
      "Maxima profundidad aplicada: 3\n",
      "[[111   8]\n",
      " [136  22]] 0.23404255319148937 0.48014440433212996\n",
      "Maxima profundidad aplicada: 4\n",
      "[[100  19]\n",
      " [118  40]] 0.3686635944700461 0.5054151624548736\n",
      "Maxima profundidad aplicada: 5\n",
      "[[75 44]\n",
      " [92 66]] 0.4925373134328358 0.5090252707581228\n",
      "Maxima profundidad aplicada: 6\n",
      "[[ 81  38]\n",
      " [102  56]] 0.4444444444444444 0.49458483754512633\n",
      "Maxima profundidad aplicada: 7\n",
      "[[67 52]\n",
      " [94 64]] 0.4671532846715329 0.4729241877256318\n",
      "Maxima profundidad aplicada: 8\n",
      "[[60 59]\n",
      " [83 75]] 0.5136986301369862 0.48736462093862815\n",
      "Maxima profundidad aplicada: 9\n",
      "[[65 54]\n",
      " [92 66]] 0.47482014388489213 0.4729241877256318\n",
      "Maxima profundidad aplicada: 10\n",
      "[[62 57]\n",
      " [88 70]] 0.49122807017543857 0.47653429602888087\n",
      "Maxima profundidad aplicada: 11\n",
      "[[67 52]\n",
      " [86 72]] 0.5106382978723404 0.5018050541516246\n",
      "Maxima profundidad aplicada: 12\n",
      "[[59 60]\n",
      " [84 74]] 0.5068493150684932 0.48014440433212996\n",
      "Maxima profundidad aplicada: 13\n",
      "[[60 59]\n",
      " [80 78]] 0.5288135593220339 0.4981949458483754\n",
      "Maxima profundidad aplicada: 14\n",
      "[[59 60]\n",
      " [79 79]] 0.531986531986532 0.4981949458483754\n",
      "Maxima profundidad aplicada: 15\n",
      "[[56 63]\n",
      " [79 79]] 0.5266666666666667 0.48736462093862815\n",
      "Maxima profundidad aplicada: 16\n",
      "[[59 60]\n",
      " [86 72]] 0.496551724137931 0.4729241877256318\n",
      "Maxima profundidad aplicada: 17\n",
      "[[58 61]\n",
      " [82 76]] 0.5152542372881356 0.48375451263537905\n",
      "Maxima profundidad aplicada: 18\n",
      "[[60 59]\n",
      " [80 78]] 0.5288135593220339 0.4981949458483754\n",
      "Maxima profundidad aplicada: 19\n",
      "[[61 58]\n",
      " [83 75]] 0.5154639175257731 0.49097472924187724\n",
      "Maxima profundidad aplicada: 20\n",
      "[[60 59]\n",
      " [84 74]] 0.5085910652920962 0.48375451263537905\n",
      "Maxima profundidad aplicada: 21\n",
      "[[59 60]\n",
      " [85 73]] 0.5017182130584192 0.47653429602888087\n",
      "Maxima profundidad aplicada: 22\n",
      "[[59 60]\n",
      " [88 70]] 0.48611111111111105 0.4657039711191336\n",
      "Maxima profundidad aplicada: 23\n",
      "[[63 56]\n",
      " [90 68]] 0.48226950354609927 0.4729241877256318\n",
      "Maxima profundidad aplicada: 24\n",
      "[[60 59]\n",
      " [85 73]] 0.503448275862069 0.48014440433212996\n",
      "Maxima profundidad aplicada: 25\n",
      "[[57 62]\n",
      " [88 70]] 0.4827586206896552 0.4584837545126354\n",
      "Maxima profundidad aplicada: 26\n",
      "[[59 60]\n",
      " [85 73]] 0.5017182130584192 0.47653429602888087\n",
      "Maxima profundidad aplicada: 27\n",
      "[[56 63]\n",
      " [83 75]] 0.5067567567567567 0.4729241877256318\n",
      "Maxima profundidad aplicada: 28\n",
      "[[59 60]\n",
      " [89 69]] 0.4808362369337979 0.4620938628158845\n",
      "Maxima profundidad aplicada: 29\n",
      "[[62 57]\n",
      " [80 78]] 0.5324232081911263 0.5054151624548736\n",
      "Maxima profundidad aplicada: 30\n",
      "[[55 64]\n",
      " [85 73]] 0.49491525423728816 0.4620938628158845\n"
     ]
    }
   ],
   "source": [
    "for profund in range(1,31):\n",
    "    dt = DecisionTreeClassifier(max_depth=profund)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(dt, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Maxima profundidad aplicada: \" + str(profund))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimadores aplicados: 1\n",
      "[[ 98  21]\n",
      " [112  46]] 0.4088888888888889 0.51985559566787\n",
      "Estimadores aplicados: 2\n",
      "[[ 96  23]\n",
      " [110  48]] 0.4192139737991266 0.51985559566787\n",
      "Estimadores aplicados: 3\n",
      "[[ 91  28]\n",
      " [109  49]] 0.41702127659574467 0.5054151624548736\n",
      "Estimadores aplicados: 4\n",
      "[[77 42]\n",
      " [86 72]] 0.5294117647058824 0.5379061371841155\n",
      "Estimadores aplicados: 5\n",
      "[[73 46]\n",
      " [92 66]] 0.4888888888888889 0.5018050541516246\n",
      "Estimadores aplicados: 6\n",
      "[[71 48]\n",
      " [74 84]] 0.5793103448275861 0.5595667870036101\n",
      "Estimadores aplicados: 7\n",
      "[[71 48]\n",
      " [87 71]] 0.5126353790613719 0.5126353790613718\n",
      "Estimadores aplicados: 8\n",
      "[[66 53]\n",
      " [82 76]] 0.529616724738676 0.5126353790613718\n",
      "Estimadores aplicados: 9\n",
      "[[70 49]\n",
      " [81 77]] 0.5422535211267605 0.5306859205776173\n",
      "Estimadores aplicados: 10\n",
      "[[69 50]\n",
      " [70 88]] 0.5945945945945945 0.5667870036101083\n",
      "Estimadores aplicados: 11\n",
      "[[68 51]\n",
      " [76 82]] 0.563573883161512 0.5415162454873647\n",
      "Estimadores aplicados: 12\n",
      "[[64 55]\n",
      " [79 79]] 0.541095890410959 0.516245487364621\n",
      "Estimadores aplicados: 13\n",
      "[[71 48]\n",
      " [79 79]] 0.5543859649122806 0.5415162454873647\n",
      "Estimadores aplicados: 14\n",
      "[[65 54]\n",
      " [76 82]] 0.5578231292517007 0.5306859205776173\n",
      "Estimadores aplicados: 15\n",
      "[[67 52]\n",
      " [84 74]] 0.5211267605633804 0.5090252707581228\n",
      "Estimadores aplicados: 16\n",
      "[[67 52]\n",
      " [84 74]] 0.5211267605633804 0.5090252707581228\n",
      "Estimadores aplicados: 17\n",
      "[[73 46]\n",
      " [79 79]] 0.558303886925795 0.5487364620938628\n",
      "Estimadores aplicados: 18\n",
      "[[71 48]\n",
      " [78 80]] 0.5594405594405595 0.5451263537906137\n",
      "Estimadores aplicados: 19\n",
      "[[75 44]\n",
      " [78 80]] 0.5673758865248227 0.5595667870036101\n",
      "Estimadores aplicados: 20\n",
      "[[68 51]\n",
      " [80 78]] 0.5435540069686412 0.5270758122743683\n"
     ]
    }
   ],
   "source": [
    "for estimadores in range(1,21):\n",
    "    adab = AdaBoostClassifier(n_estimators=estimadores)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(adab, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Estimadores aplicados: \" + str(estimadores))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[514, 159],\n",
       "        [451, 259]]),\n",
       " 0.45921985815602834,\n",
       " 0.5589298626174982)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "dt.fit(X_train_c,y_train_c)\n",
    "y_predict=dt.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[388, 285],\n",
       "        [  0, 710]]),\n",
       " 0.8328445747800587,\n",
       " 0.7939262472885033)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = SVC(probability=True, kernel='poly', degree=3)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "sv.fit(X_train_c,y_train_c)\n",
    "y_predict=sv.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[440, 233],\n",
       "        [341, 369]]),\n",
       " 0.5624999999999999,\n",
       " 0.5849602313810557)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adab = AdaBoostClassifier(n_estimators=6)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "adab.fit(X_train_c,y_train_c)\n",
    "y_predict=adab.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[350, 323],\n",
       "        [217, 493]]),\n",
       " 0.6461336828309304,\n",
       " 0.6095444685466378)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adab = AdaBoostClassifier(n_estimators=10)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "adab.fit(X_train_c,y_train_c)\n",
    "y_predict=adab.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[417, 256],\n",
       "        [233, 477]]),\n",
       " 0.661122661122661,\n",
       " 0.6464208242950108)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adab = AdaBoostClassifier(n_estimators=19)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "adab.fit(X_train_c,y_train_c)\n",
    "y_predict=adab.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[430, 243],\n",
       "        [262, 448]]),\n",
       " 0.6395431834403997,\n",
       " 0.6348517715112075)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "kn.fit(X_train_c,y_train_c)\n",
    "y_predict=kn.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[415, 258],\n",
       "        [238, 472]]),\n",
       " 0.6555555555555556,\n",
       " 0.6413593637020969)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "kn.fit(X_train_c,y_train_c)\n",
    "y_predict=kn.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 68  82]\n",
      " [ 70 126]] 0.6237623762376238 0.5606936416184971\n"
     ]
    }
   ],
   "source": [
    "#Este es el código para ver cómo funciona el modelo de verdad (este es el resultado del basic)\n",
    "\n",
    "adab = AdaBoostClassifier(n_estimators=10)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "X_test_c=X_test.copy()\n",
    "y_test_c=y_test.copy()\n",
    "\n",
    "conf_mat, f1sc, accsc = walk_forward_validation(adab, X_train_c, y_train_c, X_test_c, y_test_c)\n",
    "\n",
    "print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
