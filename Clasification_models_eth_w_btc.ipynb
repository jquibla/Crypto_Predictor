{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import numbers\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_for_use = pd.read_csv('./ethereum_data_for_use.csv', index_col=0)\n",
    "data_btc = pd.read_csv('./bitcoin_data_for_use_v2.csv', index_col=0)\n",
    "data_gold = pd.read_csv('./gold_data_for_use.csv', index_col=0)\n",
    "data_nasdaq = pd.read_csv('./nasdaq_data_for_use.csv', index_col=0)\n",
    "\n",
    "#los datos de volumen y el dia del bitcoin no nos interesan para cruzar con ETH\n",
    "data_btc = data_btc.drop(labels=['dia', 'subida',\n",
    "                    'varV0','varV1','varV2','varV3','varV4','varV5','varV6','varV7','varV8','varV9',\n",
    "                    'varV10','varV11','varV12','varV13','varV14','varV15','varV16','varV17','varV18','varV19',\n",
    "                    'varV20','varV21','varV22','varV23','varV24','varV25','varV26','varV27','varV28','varV29'\n",
    "                    ], axis=1)\n",
    "\n",
    "data_for_use_basic = data_for_use.drop(labels=['dia',\n",
    "                    'varV0','varV1','varV2','varV3','varV4','varV5','varV6','varV7','varV8','varV9',\n",
    "                    'varV10','varV11','varV12','varV13','varV14','varV15','varV16','varV17','varV18','varV19',\n",
    "                    'varV20','varV21','varV22','varV23','varV24','varV25','varV26','varV27','varV28','varV29'\n",
    "                    ], axis=1)\n",
    "\n",
    "\n",
    "data_for_use_w_gold = data_for_use.merge(data_gold, left_index=True, right_index=True)\n",
    "data_for_use_w_nasdaq = data_for_use.merge(data_nasdaq, left_index=True, right_index=True)\n",
    "\n",
    "data_for_use_w_btc = data_for_use_basic.merge(data_btc, left_index=True, right_index=True)\n",
    "\n",
    "data_for_use_w_all = data_for_use_w_gold.merge(data_nasdaq, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "data_for_use_bone_deep = data_for_use_basic.drop(labels=['varPm93',\n",
    "                    'varPm123','varPm153','varPm183','varPm213','varPm243','varPm273','varPm303','varPm333',\n",
    "                    'varPs30','varPs37','varPs44','varPs51','varPs58','varPs65','varPs72','varPs79','varPs86'\n",
    "                    ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varP0_x</th>\n",
       "      <th>varP1_x</th>\n",
       "      <th>varP2_x</th>\n",
       "      <th>varP3_x</th>\n",
       "      <th>varP4_x</th>\n",
       "      <th>varP5_x</th>\n",
       "      <th>varP6_x</th>\n",
       "      <th>varP7_x</th>\n",
       "      <th>varP8_x</th>\n",
       "      <th>varP9_x</th>\n",
       "      <th>...</th>\n",
       "      <th>varPs86_y</th>\n",
       "      <th>varPm93_y</th>\n",
       "      <th>varPm123_y</th>\n",
       "      <th>varPm153_y</th>\n",
       "      <th>varPm183_y</th>\n",
       "      <th>varPm213_y</th>\n",
       "      <th>varPm243_y</th>\n",
       "      <th>varPm273_y</th>\n",
       "      <th>varPm303_y</th>\n",
       "      <th>varPm333_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>-0.012482</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>-0.055221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012113</td>\n",
       "      <td>0.059539</td>\n",
       "      <td>-0.050802</td>\n",
       "      <td>-0.138454</td>\n",
       "      <td>0.308064</td>\n",
       "      <td>0.194686</td>\n",
       "      <td>0.069850</td>\n",
       "      <td>0.024249</td>\n",
       "      <td>0.110799</td>\n",
       "      <td>-0.145794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>-0.012482</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.072829</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>-0.168825</td>\n",
       "      <td>0.157187</td>\n",
       "      <td>0.274157</td>\n",
       "      <td>0.059979</td>\n",
       "      <td>0.052111</td>\n",
       "      <td>0.028173</td>\n",
       "      <td>-0.098079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>0.160737</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>-0.012482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>0.061430</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>-0.171586</td>\n",
       "      <td>0.193696</td>\n",
       "      <td>0.278477</td>\n",
       "      <td>0.056472</td>\n",
       "      <td>0.040036</td>\n",
       "      <td>0.054736</td>\n",
       "      <td>-0.099174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>0.156947</td>\n",
       "      <td>0.160737</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.013799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011208</td>\n",
       "      <td>0.023271</td>\n",
       "      <td>0.034452</td>\n",
       "      <td>-0.137689</td>\n",
       "      <td>0.166355</td>\n",
       "      <td>0.251030</td>\n",
       "      <td>0.085472</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>-0.177986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>-0.088627</td>\n",
       "      <td>0.156947</td>\n",
       "      <td>0.160737</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.058415</td>\n",
       "      <td>-0.151016</td>\n",
       "      <td>0.156769</td>\n",
       "      <td>0.276971</td>\n",
       "      <td>0.084663</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>0.099177</td>\n",
       "      <td>-0.169031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21</th>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>-0.036545</td>\n",
       "      <td>0.042766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029375</td>\n",
       "      <td>-0.043061</td>\n",
       "      <td>-0.307969</td>\n",
       "      <td>-0.011426</td>\n",
       "      <td>-0.027993</td>\n",
       "      <td>0.819894</td>\n",
       "      <td>0.296122</td>\n",
       "      <td>0.294667</td>\n",
       "      <td>0.420558</td>\n",
       "      <td>0.264591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-22</th>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>-0.036545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087056</td>\n",
       "      <td>-0.156113</td>\n",
       "      <td>-0.274826</td>\n",
       "      <td>-0.054379</td>\n",
       "      <td>-0.048679</td>\n",
       "      <td>0.743330</td>\n",
       "      <td>0.420131</td>\n",
       "      <td>0.265584</td>\n",
       "      <td>0.400979</td>\n",
       "      <td>0.219860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-23</th>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103432</td>\n",
       "      <td>-0.065139</td>\n",
       "      <td>-0.319474</td>\n",
       "      <td>-0.031845</td>\n",
       "      <td>-0.026437</td>\n",
       "      <td>0.690406</td>\n",
       "      <td>0.351016</td>\n",
       "      <td>0.242234</td>\n",
       "      <td>0.466289</td>\n",
       "      <td>0.217606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-24</th>\n",
       "      <td>-0.070941</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>-0.128739</td>\n",
       "      <td>-0.226667</td>\n",
       "      <td>-0.031976</td>\n",
       "      <td>0.058981</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.309128</td>\n",
       "      <td>0.316711</td>\n",
       "      <td>0.432640</td>\n",
       "      <td>0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-25</th>\n",
       "      <td>-0.002082</td>\n",
       "      <td>-0.070941</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.113382</td>\n",
       "      <td>-0.065753</td>\n",
       "      <td>-0.111266</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.048374</td>\n",
       "      <td>-0.012167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031455</td>\n",
       "      <td>-0.097385</td>\n",
       "      <td>-0.216349</td>\n",
       "      <td>-0.111232</td>\n",
       "      <td>0.109284</td>\n",
       "      <td>0.535708</td>\n",
       "      <td>0.224282</td>\n",
       "      <td>0.541462</td>\n",
       "      <td>0.256068</td>\n",
       "      <td>0.267280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1729 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             varP0_x   varP1_x   varP2_x   varP3_x   varP4_x   varP5_x  \\\n",
       "2017-01-01  0.025532 -0.023392 -0.014990  0.101601  0.048710 -0.014197   \n",
       "2017-01-02  0.025199  0.025532 -0.023392 -0.014990  0.101601  0.048710   \n",
       "2017-01-03  0.160737  0.025199  0.025532 -0.023392 -0.014990  0.101601   \n",
       "2017-01-04  0.156947  0.160737  0.025199  0.025532 -0.023392 -0.014990   \n",
       "2017-01-05 -0.088627  0.156947  0.160737  0.025199  0.025532 -0.023392   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2021-09-21 -0.065753 -0.111266 -0.029886  0.009851 -0.048374 -0.012167   \n",
       "2021-09-22  0.113382 -0.065753 -0.111266 -0.029886  0.009851 -0.048374   \n",
       "2021-09-23  0.025230  0.113382 -0.065753 -0.111266 -0.029886  0.009851   \n",
       "2021-09-24 -0.070941  0.025230  0.113382 -0.065753 -0.111266 -0.029886   \n",
       "2021-09-25 -0.002082 -0.070941  0.025230  0.113382 -0.065753 -0.111266   \n",
       "\n",
       "             varP6_x   varP7_x   varP8_x   varP9_x  ...  varPs86_y  varPm93_y  \\\n",
       "2017-01-01  0.013799 -0.012482  0.013901 -0.055221  ...   0.012113   0.059539   \n",
       "2017-01-02 -0.014197  0.013799 -0.012482  0.013901  ...   0.008347   0.072829   \n",
       "2017-01-03  0.048710 -0.014197  0.013799 -0.012482  ...   0.009593   0.061430   \n",
       "2017-01-04  0.101601  0.048710 -0.014197  0.013799  ...   0.011208   0.023271   \n",
       "2017-01-05 -0.014990  0.101601  0.048710 -0.014197  ...   0.050586   0.002580   \n",
       "...              ...       ...       ...       ...  ...        ...        ...   \n",
       "2021-09-21  0.054273  0.043725 -0.036545  0.042766  ...  -0.029375  -0.043061   \n",
       "2021-09-22 -0.012167  0.054273  0.043725 -0.036545  ...   0.087056  -0.156113   \n",
       "2021-09-23 -0.048374 -0.012167  0.054273  0.043725  ...   0.103432  -0.065139   \n",
       "2021-09-24  0.009851 -0.048374 -0.012167  0.054273  ...   0.039077  -0.128739   \n",
       "2021-09-25 -0.029886  0.009851 -0.048374 -0.012167  ...  -0.031455  -0.097385   \n",
       "\n",
       "            varPm123_y  varPm153_y  varPm183_y  varPm213_y  varPm243_y  \\\n",
       "2017-01-01   -0.050802   -0.138454    0.308064    0.194686    0.069850   \n",
       "2017-01-02    0.045369   -0.168825    0.157187    0.274157    0.059979   \n",
       "2017-01-03    0.016212   -0.171586    0.193696    0.278477    0.056472   \n",
       "2017-01-04    0.034452   -0.137689    0.166355    0.251030    0.085472   \n",
       "2017-01-05    0.058415   -0.151016    0.156769    0.276971    0.084663   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2021-09-21   -0.307969   -0.011426   -0.027993    0.819894    0.296122   \n",
       "2021-09-22   -0.274826   -0.054379   -0.048679    0.743330    0.420131   \n",
       "2021-09-23   -0.319474   -0.031845   -0.026437    0.690406    0.351016   \n",
       "2021-09-24   -0.226667   -0.031976    0.058981    0.512089    0.309128   \n",
       "2021-09-25   -0.216349   -0.111232    0.109284    0.535708    0.224282   \n",
       "\n",
       "            varPm273_y  varPm303_y  varPm333_y  \n",
       "2017-01-01    0.024249    0.110799   -0.145794  \n",
       "2017-01-02    0.052111    0.028173   -0.098079  \n",
       "2017-01-03    0.040036    0.054736   -0.099174  \n",
       "2017-01-04    0.021944    0.100390   -0.177986  \n",
       "2017-01-05    0.021192    0.099177   -0.169031  \n",
       "...                ...         ...         ...  \n",
       "2021-09-21    0.294667    0.420558    0.264591  \n",
       "2021-09-22    0.265584    0.400979    0.219860  \n",
       "2021-09-23    0.242234    0.466289    0.217606  \n",
       "2021-09-24    0.316711    0.432640    0.215800  \n",
       "2021-09-25    0.541462    0.256068    0.267280  \n",
       "\n",
       "[1729 rows x 97 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_use_w_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui se marca lo que se va a usar de verdad para el resto del programa\n",
    "data_for_use = data_for_use_w_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partimos los datos en train y test por fechas, sin que sea aleatorio, que es como funcionaría\n",
    "# en la realidad el modelo\n",
    "data_no_test, data_test = train_test_split(data_for_use, shuffle=False, test_size=0.2)\n",
    "\n",
    "# Partimos los datos de train nuevamente en validation y train\n",
    "data_train, data_valid = train_test_split(data_no_test, shuffle=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos la variable que queremos predecir\n",
    "X_train=data_train.drop(labels=['subida'], axis=1)\n",
    "y_train=data_train['subida']\n",
    "\n",
    "X_valid=data_valid.drop(labels=['subida'], axis=1)\n",
    "y_valid=data_valid['subida']\n",
    "\n",
    "X_test=data_test.drop(labels=['subida'], axis=1)\n",
    "y_test=data_test['subida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49909584086799275"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Miramos cuántos días sube el eth sobre el total en cada uno de los segmentos\n",
    "# esto nos servirá para hacernos una idea de cómo de buenos son los resultados de los modelos\n",
    "y_train.sum()/y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5703971119133574"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.sum()/y_valid.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5664739884393064"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()/y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenemos que hacer one-hot encoding sobre la columna \"dia\"\n",
    "#Sobre el resto aplicamos el standardscaler\n",
    "\n",
    "number_columns = data_for_use.select_dtypes('number').columns\n",
    "\n",
    "t=[\n",
    "    #('dia', \n",
    "    #OneHotEncoder(handle_unknown='ignore'),\n",
    "    #['dia']),\n",
    "    ('scaler', StandardScaler(),number_columns)\n",
    "    ]\n",
    "\n",
    "ct = ColumnTransformer(transformers=t, remainder='passthrough')\n",
    "\n",
    "#data_for_use_t=ct.fit_transform(data_for_use)\n",
    "\n",
    "#X_train=ct.fit_transform(X_train)\n",
    "#X_valid=ct.transform(X_valid)\n",
    "#X_test=ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation (model, X_train_wfv, y_train_wfv, X_test_wfv, y_test_wfv):\n",
    "    y_pred_wfv = list()\n",
    "    for i in range(len(y_test_wfv)):\n",
    "        X_train_wfv_ct=ct.fit_transform(X_train_wfv)\n",
    "        model.fit(X_train_wfv_ct, y_train_wfv)\n",
    "        X_test_wfv_ct=ct.transform(X_test_wfv)\n",
    "        y_pred_next = model.predict(X_test_wfv_ct[i:i+1])\n",
    "        y_pred_wfv.append(y_pred_next[0])\n",
    "        X_train_wfv=X_train_wfv.append(X_test_wfv[i:i+1])\n",
    "        y_train_wfv=y_train_wfv.append(pd.Series(y_test_wfv[i]))\n",
    "    \n",
    "    return metrics.confusion_matrix(y_test_wfv, y_pred_wfv), metrics.f1_score(y_test_wfv,y_pred_wfv), metrics.accuracy_score(y_test_wfv,y_pred_wfv)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "\n",
    "X_train_c=X_train.copy()\n",
    "y_train_c=y_train.copy()\n",
    "X_valid_c=X_valid.copy()\n",
    "y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "conf_mat, f1sc, accsc = walk_forward_validation(lr, X_train_c, y_train_c, X_valid_c, y_valid_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59 60]\n",
      " [79 79]] 0.531986531986532 0.4981949458483754\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de vecinos aplicado: 1\n",
      "[[45 74]\n",
      " [79 79]] 0.5080385852090032 0.44765342960288806\n",
      "Numero de vecinos aplicado: 2\n",
      "[[ 78  41]\n",
      " [106  52]] 0.4143426294820717 0.4693140794223827\n",
      "Numero de vecinos aplicado: 3\n",
      "[[47 72]\n",
      " [71 87]] 0.5488958990536278 0.48375451263537905\n",
      "Numero de vecinos aplicado: 4\n",
      "[[ 69  50]\n",
      " [102  56]] 0.42424242424242425 0.45126353790613716\n",
      "Numero de vecinos aplicado: 5\n",
      "[[56 63]\n",
      " [69 89]] 0.5741935483870967 0.5234657039711191\n",
      "Numero de vecinos aplicado: 6\n",
      "[[75 44]\n",
      " [95 63]] 0.47547169811320755 0.4981949458483754\n",
      "Numero de vecinos aplicado: 7\n",
      "[[48 71]\n",
      " [63 95]] 0.5864197530864197 0.516245487364621\n",
      "Numero de vecinos aplicado: 8\n",
      "[[68 51]\n",
      " [80 78]] 0.5435540069686412 0.5270758122743683\n",
      "Numero de vecinos aplicado: 9\n",
      "[[48 71]\n",
      " [60 98]] 0.5993883792048929 0.5270758122743683\n",
      "Numero de vecinos aplicado: 10\n",
      "[[62 57]\n",
      " [82 76]] 0.5223367697594502 0.4981949458483754\n",
      "Numero de vecinos aplicado: 11\n",
      "[[47 72]\n",
      " [63 95]] 0.5846153846153846 0.5126353790613718\n",
      "Numero de vecinos aplicado: 12\n",
      "[[55 64]\n",
      " [81 77]] 0.5150501672240803 0.47653429602888087\n",
      "Numero de vecinos aplicado: 13\n",
      "[[44 75]\n",
      " [61 97]] 0.5878787878787879 0.5090252707581228\n",
      "Numero de vecinos aplicado: 14\n",
      "[[59 60]\n",
      " [72 86]] 0.5657894736842106 0.5234657039711191\n",
      "Numero de vecinos aplicado: 15\n",
      "[[ 45  74]\n",
      " [ 55 103]] 0.6149253731343283 0.5342960288808665\n",
      "Numero de vecinos aplicado: 16\n",
      "[[59 60]\n",
      " [72 86]] 0.5657894736842106 0.5234657039711191\n",
      "Numero de vecinos aplicado: 17\n",
      "[[ 45  74]\n",
      " [ 51 107]] 0.6312684365781711 0.5487364620938628\n",
      "Numero de vecinos aplicado: 18\n",
      "[[57 62]\n",
      " [64 94]] 0.5987261146496815 0.5451263537906137\n",
      "Numero de vecinos aplicado: 19\n",
      "[[ 42  77]\n",
      " [ 50 108]] 0.6297376093294461 0.5415162454873647\n",
      "Numero de vecinos aplicado: 20\n",
      "[[54 65]\n",
      " [67 91]] 0.5796178343949043 0.5234657039711191\n",
      "Numero de vecinos aplicado: 21\n",
      "[[ 44  75]\n",
      " [ 57 101]] 0.6047904191616766 0.5234657039711191\n",
      "Numero de vecinos aplicado: 22\n",
      "[[54 65]\n",
      " [67 91]] 0.5796178343949043 0.5234657039711191\n",
      "Numero de vecinos aplicado: 23\n",
      "[[ 43  76]\n",
      " [ 52 106]] 0.6235294117647059 0.5379061371841155\n",
      "Numero de vecinos aplicado: 24\n",
      "[[53 66]\n",
      " [68 90]] 0.573248407643312 0.516245487364621\n",
      "Numero de vecinos aplicado: 25\n",
      "[[34 85]\n",
      " [60 98]] 0.5747800586510264 0.47653429602888087\n",
      "Numero de vecinos aplicado: 26\n",
      "[[46 73]\n",
      " [71 87]] 0.5471698113207547 0.48014440433212996\n",
      "Numero de vecinos aplicado: 27\n",
      "[[ 33  86]\n",
      " [ 58 100]] 0.5813953488372092 0.48014440433212996\n",
      "Numero de vecinos aplicado: 28\n",
      "[[43 76]\n",
      " [69 89]] 0.5510835913312693 0.47653429602888087\n",
      "Numero de vecinos aplicado: 29\n",
      "[[ 29  90]\n",
      " [ 56 102]] 0.5828571428571429 0.4729241877256318\n",
      "Numero de vecinos aplicado: 30\n",
      "[[42 77]\n",
      " [66 92]] 0.5626911314984709 0.48375451263537905\n"
     ]
    }
   ],
   "source": [
    "for vecinos in range(1,31):\n",
    "    kn = KNeighborsClassifier(n_neighbors=vecinos)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(kn, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Numero de vecinos aplicado: \" + str(vecinos))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grado aplicado: 1\n",
      "[[64 55]\n",
      " [73 85]] 0.5704697986577181 0.5379061371841155\n",
      "Grado aplicado: 2\n",
      "[[37 82]\n",
      " [59 99]] 0.584070796460177 0.49097472924187724\n",
      "Grado aplicado: 3\n",
      "[[44 75]\n",
      " [60 98]] 0.5921450151057402 0.5126353790613718\n"
     ]
    }
   ],
   "source": [
    "for grado in range(1,4):\n",
    "    sv = SVC(probability=True, kernel='poly', degree=grado)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(sv, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Grado aplicado: \" + str(grado))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxima profundidad aplicada: 1\n",
      "[[ 98  21]\n",
      " [112  46]] 0.4088888888888889 0.51985559566787\n",
      "Maxima profundidad aplicada: 2\n",
      "[[ 83  36]\n",
      " [100  58]] 0.4603174603174603 0.5090252707581228\n",
      "Maxima profundidad aplicada: 3\n",
      "[[ 99  20]\n",
      " [125  33]] 0.31279620853080564 0.47653429602888087\n",
      "Maxima profundidad aplicada: 4\n",
      "[[ 93  26]\n",
      " [116  42]] 0.3716814159292035 0.48736462093862815\n",
      "Maxima profundidad aplicada: 5\n",
      "[[ 90  29]\n",
      " [111  47]] 0.40170940170940167 0.49458483754512633\n",
      "Maxima profundidad aplicada: 6\n",
      "[[71 48]\n",
      " [87 71]] 0.5126353790613719 0.5126353790613718\n",
      "Maxima profundidad aplicada: 7\n",
      "[[80 39]\n",
      " [84 74]] 0.5461254612546126 0.555956678700361\n",
      "Maxima profundidad aplicada: 8\n",
      "[[65 54]\n",
      " [81 77]] 0.5328719723183392 0.5126353790613718\n",
      "Maxima profundidad aplicada: 9\n",
      "[[71 48]\n",
      " [81 77]] 0.5441696113074205 0.5342960288808665\n",
      "Maxima profundidad aplicada: 10\n",
      "[[63 56]\n",
      " [75 83]] 0.5589225589225589 0.5270758122743683\n",
      "Maxima profundidad aplicada: 11\n",
      "[[65 54]\n",
      " [79 79]] 0.5429553264604811 0.51985559566787\n",
      "Maxima profundidad aplicada: 12\n",
      "[[59 60]\n",
      " [83 75]] 0.5119453924914675 0.48375451263537905\n",
      "Maxima profundidad aplicada: 13\n",
      "[[69 50]\n",
      " [78 80]] 0.5555555555555556 0.5379061371841155\n",
      "Maxima profundidad aplicada: 14\n",
      "[[68 51]\n",
      " [83 75]] 0.528169014084507 0.516245487364621\n",
      "Maxima profundidad aplicada: 15\n",
      "[[66 53]\n",
      " [78 80]] 0.5498281786941581 0.5270758122743683\n",
      "Maxima profundidad aplicada: 16\n",
      "[[68 51]\n",
      " [79 79]] 0.548611111111111 0.5306859205776173\n",
      "Maxima profundidad aplicada: 17\n",
      "[[65 54]\n",
      " [74 84]] 0.5675675675675675 0.5379061371841155\n",
      "Maxima profundidad aplicada: 18\n",
      "[[65 54]\n",
      " [74 84]] 0.5675675675675675 0.5379061371841155\n",
      "Maxima profundidad aplicada: 19\n",
      "[[59 60]\n",
      " [79 79]] 0.531986531986532 0.4981949458483754\n",
      "Maxima profundidad aplicada: 20\n",
      "[[64 55]\n",
      " [76 82]] 0.5559322033898305 0.5270758122743683\n",
      "Maxima profundidad aplicada: 21\n",
      "[[64 55]\n",
      " [74 84]] 0.5656565656565656 0.5342960288808665\n",
      "Maxima profundidad aplicada: 22\n",
      "[[67 52]\n",
      " [75 83]] 0.5665529010238908 0.5415162454873647\n",
      "Maxima profundidad aplicada: 23\n",
      "[[62 57]\n",
      " [76 82]] 0.5521885521885521 0.51985559566787\n",
      "Maxima profundidad aplicada: 24\n",
      "[[63 56]\n",
      " [78 80]] 0.5442176870748299 0.516245487364621\n",
      "Maxima profundidad aplicada: 25\n",
      "[[65 54]\n",
      " [73 85]] 0.5723905723905723 0.5415162454873647\n",
      "Maxima profundidad aplicada: 26\n",
      "[[61 58]\n",
      " [75 83]] 0.5551839464882943 0.51985559566787\n",
      "Maxima profundidad aplicada: 27\n",
      "[[60 59]\n",
      " [81 77]] 0.5238095238095238 0.49458483754512633\n",
      "Maxima profundidad aplicada: 28\n",
      "[[62 57]\n",
      " [78 80]] 0.5423728813559322 0.5126353790613718\n",
      "Maxima profundidad aplicada: 29\n",
      "[[68 51]\n",
      " [79 79]] 0.548611111111111 0.5306859205776173\n",
      "Maxima profundidad aplicada: 30\n",
      "[[64 55]\n",
      " [79 79]] 0.541095890410959 0.516245487364621\n"
     ]
    }
   ],
   "source": [
    "for profund in range(1,31):\n",
    "    dt = DecisionTreeClassifier(max_depth=profund)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(dt, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Maxima profundidad aplicada: \" + str(profund))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimadores aplicados: 1\n",
      "[[ 98  21]\n",
      " [112  46]] 0.4088888888888889 0.51985559566787\n",
      "Estimadores aplicados: 2\n",
      "[[ 97  22]\n",
      " [111  47]] 0.41409691629955947 0.51985559566787\n",
      "Estimadores aplicados: 3\n",
      "[[ 93  26]\n",
      " [107  51]] 0.4340425531914893 0.51985559566787\n",
      "Estimadores aplicados: 4\n",
      "[[ 92  27]\n",
      " [107  51]] 0.43220338983050843 0.516245487364621\n",
      "Estimadores aplicados: 5\n",
      "[[59 60]\n",
      " [75 83]] 0.5514950166112956 0.5126353790613718\n",
      "Estimadores aplicados: 6\n",
      "[[ 81  38]\n",
      " [105  53]] 0.42570281124497994 0.48375451263537905\n",
      "Estimadores aplicados: 7\n",
      "[[60 59]\n",
      " [80 78]] 0.5288135593220339 0.4981949458483754\n",
      "Estimadores aplicados: 8\n",
      "[[62 57]\n",
      " [81 77]] 0.5273972602739727 0.5018050541516246\n",
      "Estimadores aplicados: 9\n",
      "[[64 55]\n",
      " [76 82]] 0.5559322033898305 0.5270758122743683\n",
      "Estimadores aplicados: 10\n",
      "[[60 59]\n",
      " [83 75]] 0.5136986301369862 0.48736462093862815\n",
      "Estimadores aplicados: 11\n",
      "[[67 52]\n",
      " [81 77]] 0.5365853658536586 0.51985559566787\n",
      "Estimadores aplicados: 12\n",
      "[[64 55]\n",
      " [72 86]] 0.5752508361204013 0.5415162454873647\n",
      "Estimadores aplicados: 13\n",
      "[[67 52]\n",
      " [66 92]] 0.609271523178808 0.5740072202166066\n",
      "Estimadores aplicados: 14\n",
      "[[61 58]\n",
      " [68 90]] 0.5882352941176471 0.5451263537906137\n",
      "Estimadores aplicados: 15\n",
      "[[60 59]\n",
      " [71 87]] 0.5723684210526315 0.5306859205776173\n",
      "Estimadores aplicados: 16\n",
      "[[61 58]\n",
      " [70 88]] 0.5789473684210525 0.5379061371841155\n",
      "Estimadores aplicados: 17\n",
      "[[59 60]\n",
      " [73 85]] 0.5610561056105611 0.51985559566787\n",
      "Estimadores aplicados: 18\n",
      "[[62 57]\n",
      " [69 89]] 0.5855263157894738 0.5451263537906137\n",
      "Estimadores aplicados: 19\n",
      "[[65 54]\n",
      " [71 87]] 0.5819397993311038 0.5487364620938628\n",
      "Estimadores aplicados: 20\n",
      "[[63 56]\n",
      " [72 86]] 0.5733333333333334 0.5379061371841155\n"
     ]
    }
   ],
   "source": [
    "for estimadores in range(1,21):\n",
    "    adab = AdaBoostClassifier(n_estimators=estimadores)\n",
    "    \n",
    "    X_train_c=X_train.copy()\n",
    "    y_train_c=y_train.copy()\n",
    "    X_valid_c=X_valid.copy()\n",
    "    y_valid_c=y_valid.copy()\n",
    "\n",
    "\n",
    "    conf_mat, f1sc, accsc = walk_forward_validation(adab, X_train_c, y_train_c, X_valid_c, y_valid_c)\n",
    "    print (\"Estimadores aplicados: \" + str(estimadores))\n",
    "    print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[580,  93],\n",
       "        [253, 457]]),\n",
       " 0.7253968253968255,\n",
       " 0.749819233550253)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=7)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "dt.fit(X_train_c,y_train_c)\n",
    "y_predict=dt.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[380, 293],\n",
       "        [232, 478]]),\n",
       " 0.6455097906819716,\n",
       " 0.6203904555314533)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = SVC(probability=True, kernel='poly', degree=1)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "sv.fit(X_train_c,y_train_c)\n",
    "y_predict=sv.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[450, 223],\n",
       "        [295, 415]]),\n",
       " 0.615727002967359,\n",
       " 0.6254519161243673)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adab = AdaBoostClassifier(n_estimators=13)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "adab.fit(X_train_c,y_train_c)\n",
    "y_predict=adab.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[320, 353],\n",
       "        [220, 490]]),\n",
       " 0.6310367031551835,\n",
       " 0.5856832971800434)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=17)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "kn.fit(X_train_c,y_train_c)\n",
    "y_predict=kn.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[300, 373],\n",
       "        [219, 491]]),\n",
       " 0.6238881829733165,\n",
       " 0.571945046999277)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=23)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "\n",
    "X_train_c=ct.fit_transform(X_train_c)\n",
    "kn.fit(X_train_c,y_train_c)\n",
    "y_predict=kn.predict(X_train_c)\n",
    "\n",
    "(metrics.confusion_matrix(y_train_c, y_predict), metrics.f1_score(y_train_c,y_predict), metrics.accuracy_score(y_train_c,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75  75]\n",
      " [ 87 109]] 0.5736842105263158 0.5317919075144508\n"
     ]
    }
   ],
   "source": [
    "#Este es el código para ver cómo funciona el modelo de verdad\n",
    "\n",
    "adab = AdaBoostClassifier(n_estimators=13)\n",
    "\n",
    "X_train_c=X_train.append(X_valid)\n",
    "y_train_c=y_train.append(y_valid)\n",
    "X_test_c=X_test.copy()\n",
    "y_test_c=y_test.copy()\n",
    "\n",
    "conf_mat, f1sc, accsc = walk_forward_validation(adab, X_train_c, y_train_c, X_test_c, y_test_c)\n",
    "\n",
    "print(conf_mat, f1sc, accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
